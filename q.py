import requests
import base64
import re
import socket
from urllib.parse import quote
from bs4 import BeautifulSoup
import time

# --- é…ç½®åŒº ---
TEMPLATE_FILE = "vless_template.txt"
DOMAINS_FILE = "domains.txt"
OUTPUT_FILE = "sub.txt"
FIXED_SNI_HOST = "bui.2514376.xyz"

REMOTE_IP_URL_1 = "https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/All_IPs_port_443.txt"
DYNAMIC_IP_URL = "https://stock.hostmonit.com/CloudFlareYes"
GITHUB_IP_URL = "https://raw.githubusercontent.com/qwer-search/bestip/refs/heads/main/kejilandbestip.txt"

# åœ°ç†ä½ç½®ç¼“å­˜
GEO_CACHE = {}
# --- é…ç½®åŒºç»“æŸ ---

def get_domain_location(domain):
    """é€šè¿‡åŸŸåè·å–åœ°ç†ä½ç½®ï¼ˆä¼˜åŒ–ç‰ˆï¼šæŸ¥è¯¢ASN/åŸŸåæ‰˜ç®¡ä½ç½®ï¼‰"""
    if domain in GEO_CACHE:
        return GEO_CACHE[domain]
   Â 
    try:
        # 1. è§£æåŸŸåä¸ºIP
        clean_domain = domain.split(':')[0]
        ip = socket.gethostbyname(clean_domain)
       Â 
        # 2. æŸ¥è¯¢IPçš„ASNå’Œæ‰˜ç®¡ä¿¡æ¯
        response = requests.get(
            f"http://ip-api.com/json/{ip}?fields=status,country,regionName,city,isp,as,org&lang=zh-CN",Â 
            timeout=5
        )
       Â 
        if response.status_code == 200:
            data = response.json()
            if data.get('status') == 'success':
                # ä¼˜å…ˆä½¿ç”¨ ISP/ç»„ç»‡åç§°ï¼ˆæ›´å‡†ç¡®ï¼‰
                isp = data.get('isp', '')
                org = data.get('org', '')
                asn = data.get('as', '')
               Â 
                # åˆ¤æ–­æ˜¯å¦ä¸ºCDN
                cdn_keywords = ['cloudflare', 'akamai', 'fastly', 'cloudfront', 'cdn']
                is_cdn = any(kw in isp.lower() or kw in org.lower() for kw in cdn_keywords)
               Â 
                if is_cdn:
                    # å¯¹äºCDNï¼Œä½¿ç”¨ASNç»„ç»‡å
                    location = asn.split(' ', 1)[1] if ' ' in asn else org or isp
                    print(f"  ğŸŒ {clean_domain} -> {location} (CDN)")
                else:
                    # éCDNä½¿ç”¨åŸå¸‚/åœ°åŒº
                    city = data.get('city', '')
                    region = data.get('regionName', '')
                    country = data.get('country', '')
                    location = city or region or country
                    print(f"  ğŸ“ {clean_domain} -> {location}")
               Â 
                GEO_CACHE[domain] = location
                time.sleep(0.15)  # é¿å…APIé™æµ
                return location
       Â 
        # APIå¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
        print(f"  âš ï¸  {clean_domain} ä½ç½®æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨IP: {ip}")
        GEO_CACHE[domain] = ip
        return ip
       Â 
    except Exception as e:
        print(f"  âŒ {domain} è§£æå¤±è´¥: {e}")
        # å°è¯•ä»åŸŸååç¼€çŒœæµ‹ä½ç½®
        fallback = get_location_from_domain_suffix(domain)
        GEO_CACHE[domain] = fallback
        return fallback

def get_location_from_domain_suffix(domain):
    """ä»åŸŸååç¼€æ¨æµ‹ä½ç½®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    suffix_map = {
        '.cn': 'ä¸­å›½',
        '.hk': 'é¦™æ¸¯',
        '.tw': 'å°æ¹¾',
        '.jp': 'æ—¥æœ¬',
        '.kr': 'éŸ©å›½',
        '.sg': 'æ–°åŠ å¡',
        '.us': 'ç¾å›½',
        '.uk': 'è‹±å›½',
        '.de': 'å¾·å›½',
        '.fr': 'æ³•å›½',
    }
   Â 
    for suffix, location in suffix_map.items():
        if domain.endswith(suffix):
            return location
   Â 
    return domain.split(':')[0]  # é»˜è®¤è¿”å›åŸŸåæœ¬èº«

def fetch_from_file(file_path):
    """ä»æœ¬åœ°æ–‡ä»¶è¯»å–åœ°å€åˆ—è¡¨"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
            print(f"âœ… ä»æœ¬åœ°æ–‡ä»¶ {file_path} è·å– {len(lines)} ä¸ªåŸŸåã€‚")
           Â 
            results = []
            for line in lines:
                location = get_domain_location(line)
                results.append({"address": line, "name_suffix": location})
           Â 
            return results
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        return []

def fetch_simple_ips(url):
    """ä»URLè·å–ç®€å•çš„IPåˆ—è¡¨"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        lines = [line.strip() for line in response.text.splitlines()Â 
                if line.strip() and not line.strip().startswith('#')]
        print(f"âœ… ä» {url.split('/')[-1]} è·å– {len(lines)} ä¸ªIPã€‚")
       Â 
        # ä¸ºIPåœ°å€ä¹ŸæŸ¥è¯¢ä½ç½®
        results = []
        for line in lines[:50]:  # é™åˆ¶æŸ¥è¯¢æ•°é‡é¿å…è¶…æ—¶
            location = get_ip_location(line)
            results.append({"address": line, "name_suffix": location})
       Â 
        # å‰©ä½™IPä½¿ç”¨é»˜è®¤æ ‡è¯†
        results.extend([{"address": line, "name_suffix": line}Â 
                       for line in lines[50:]])
       Â 
        return results
    except requests.RequestException as e:
        print(f"âŒ ä» {url.split('/')[-1]} è·å–IPå¤±è´¥: {e}")
        return []

def get_ip_location(ip):
    """æŸ¥è¯¢IPåœ°å€çš„åœ°ç†ä½ç½®"""
    if ip in GEO_CACHE:
        return GEO_CACHE[ip]
   Â 
    try:
        clean_ip = ip.split(':')[0]
        response = requests.get(
            f"http://ip-api.com/json/{clean_ip}?fields=city,regionName,country,isp&lang=zh-CN",
            timeout=3
        )
       Â 
        if response.status_code == 200:
            data = response.json()
            city = data.get('city', '')
            region = data.get('regionName', '')
            country = data.get('country', '')
            location = city or region or country or clean_ip
            GEO_CACHE[ip] = location
            time.sleep(0.1)
            return location
           Â 
    except Exception:
        pass
   Â 
    GEO_CACHE[ip] = ip
    return ip

def fetch_dynamic_ips(url):
    """ä» hostmonit è§£æåŠ¨æ€IPåœ°å€å’ŒISPä¿¡æ¯"""
    print("ğŸ”„ æ­£åœ¨ä» hostmonit è·å–åŠ¨æ€IP...")
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        response.raise_for_status()
       Â 
        soup = BeautifulSoup(response.content, 'lxml')
        results = []
        for row in soup.find_all('tr')[1:]:
            cols = row.find_all('td')
            if len(cols) >= 5:
                ip = cols[0].text.strip()
                isp = cols[4].text.strip().replace(' ', '')
                if ip and isp:
                    results.append({"address": ip, "name_suffix": isp})
       Â 
        print(f"âœ… æˆåŠŸä» hostmonit è·å– {len(results)} ä¸ªåŠ¨æ€IPã€‚")
        return results
    except Exception as e:
        print(f"âŒ hostmonit: è·å–æˆ–è§£æå¤±è´¥: {e}")
        return []

def fetch_github_ips(url):
    """ä» GitHub Raw URL è§£æå¸¦è‡ªå®šä¹‰ç«¯å£å’Œåç§°çš„IP"""
    print(f"ğŸ”„ æ­£åœ¨ä» GitHub è·å–ä¼˜é€‰IP...")
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
       Â 
        results = []
        regex = r'^([^:]+):(\d+)#(.*)$'
        for line in response.text.strip().split('\n'):
            match = re.match(regex, line.strip())
            if match:
                ip, port, name = match.groups()
                address = f"{ip}:{port}"
                name_suffix = name.strip() or ip
                results.append({"address": address, "name_suffix": name_suffix})
       Â 
        print(f"âœ… æˆåŠŸä» GitHub è·å– {len(results)} ä¸ªä¼˜é€‰IPã€‚")
        return results
    except requests.RequestException as e:
        print(f"âŒ GitHub: è·å–ä¼˜é€‰IPå¤±è´¥: {e}")
        return []

def generate_subscription():
    """ä¸»å‡½æ•°ï¼Œç”Ÿæˆè®¢é˜…æ–‡ä»¶"""
    try:
        with open(TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            vless_template = f.read().strip()
           Â 
            uuid_match = re.search(r'vless://([^@]+)@', vless_template)
            uuid = uuid_match.group(1) if uuid_match else ""
           Â 
            params_match = re.search(r'\?(.+)', vless_template)
            params = params_match.group(1) if params_match else ""
           Â 
            if "host=" not in params:
                params = params.rstrip("#") + f"&host={FIXED_SNI_HOST}"
            else:
                params = re.sub(r'host=[^&]+', f'host={FIXED_SNI_HOST}', params)
               Â 
            if "sni=" not in params:
                params = params.rstrip("#") + f"&sni={FIXED_SNI_HOST}"
            else:
                params = re.sub(r'sni=[^&]+', f'sni={FIXED_SNI_HOST}', params)
               Â 
    except FileNotFoundError:
        print(f"âŒ è‡´å‘½é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶ {TEMPLATE_FILE}ã€‚ç¨‹åºå·²ç»ˆæ­¢ã€‚")
        return

    print("\n--- å¼€å§‹è·å–æ‰€æœ‰èŠ‚ç‚¹åœ°å€ ---")
    domains = fetch_from_file(DOMAINS_FILE)
    simple_ips = fetch_simple_ips(REMOTE_IP_URL_1)
    dynamic_ips = fetch_dynamic_ips(DYNAMIC_IP_URL)
    github_ips = fetch_github_ips(GITHUB_IP_URL)
   Â 
    all_nodes = domains + simple_ips + dynamic_ips + github_ips
 Â 
    if not all_nodes:
        print("\nâš ï¸ è­¦å‘Š: æœªèƒ½è·å–ä»»ä½•åœ°å€ä¿¡æ¯ï¼Œæ— æ³•ç”Ÿæˆè®¢é˜…ã€‚")
        return

    node_links = []
    print("\nğŸš€ å¼€å§‹ç”ŸæˆèŠ‚ç‚¹é“¾æ¥...")
   Â 
    for i, node in enumerate(all_nodes):
        address = node["address"]
        name_suffix = node.get("name_suffix", address)
       Â 
        if ":" in address and not re.search(r'[a-zA-Z]', address.split(':')[0]):
            server_address = address
        else:
            server_address = f"{address}:443"
           Â 
        link = f"vless://{uuid}@{server_address}?{params}"
        node_name = f"CF-{name_suffix}-{i+1:03d}"
       Â 
        final_link = f"{link}#{quote(node_name)}"
        node_links.append(final_link)

    print(f"ğŸ‰ æ€»å…±ç”Ÿæˆäº† {len(node_links)} ä¸ªèŠ‚ç‚¹ã€‚")

    subscription_content = "\n".join(node_links)
    encoded_content = base64.b64encode(subscription_content.encode('utf-8')).decode('utf-8')

    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write(encoded_content)
        print(f"\nâœ¨ è®¢é˜…æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆï¼âœ¨\n   æ–‡ä»¶å: {OUTPUT_FILE}\n")
    except Exception as e:
        print(f"âŒ å†™å…¥è®¢é˜…æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    generate_subscription()
