import requests
import base64
import re
import socket
import time
from urllib.parse import quote
from bs4 import BeautifulSoup

# --- é…ç½®åŒº ---
TEMPLATE_FILE = "vless_template.txt"
DOMAINS_FILE = "domains.txt"
OUTPUT_FILE = "sub.txt"
FIXED_SNI_HOST = "bui.2514376.xyz"

REMOTE_IP_URL_1 = "https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/All_IPs_port_443.txt"
DYNAMIC_IP_URL = "https://stock.hostmonit.com/CloudFlareYes"
GITHUB_IP_URL = "https://raw.githubusercontent.com/qwer-search/bestip/refs/heads/main/kejilandbestip.txt"
EXTRA_IP_URL = "https://raw.githubusercontent.com/jyhuang201900/cfipcaiji/refs/heads/main/ip.txt"  # æ–°å¢çš„æ•°æ®æº
# --- é…ç½®åŒºç»“æŸ ---

def fetch_from_file(file_path):
    """ä»æœ¬åœ°æ–‡ä»¶è¯»å–åŸŸååˆ—è¡¨ï¼ˆç›´æ¥ç”¨åŸŸåä½œèŠ‚ç‚¹åï¼Œä¸æŸ¥åœ°åŒºï¼‰"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]
            print(f"âœ… ä»æœ¬åœ°æ–‡ä»¶ {file_path} è·å– {len(lines)} ä¸ªåŸŸåã€‚")
            return [{"address": line, "name_suffix": line} for line in lines]  # ç›´æ¥ç”¨åŸŸåæœ¬èº«
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        return []

def fetch_simple_ips(url):
    """ä»URLè·å–ç®€å•IPåˆ—è¡¨"""
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        lines = [line.strip() for line in r.text.splitlines() if line.strip() and not line.startswith('#')]
        print(f"âœ… ä» {url} è·å– {len(lines)} ä¸ªIPã€‚")
        return [{"address": line, "name_suffix": line} for line in lines]  # IPæœ¬èº«ä½œåç§°
    except Exception as e:
        print(f"âŒ è·å– {url} å¤±è´¥: {e}")
        return []

def fetch_dynamic_ips(url):
    """ä» hostmonit è·å–åŠ¨æ€IPåœ°å€å’ŒISPä¿¡æ¯"""
    print("ğŸ”„ è·å–åŠ¨æ€IP...")
    try:
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "lxml")
        results = []
        for row in soup.find_all("tr")[1:]:
            cols = row.find_all("td")
            if len(cols) >= 5:
                ip = cols[0].text.strip()
                isp = cols[4].text.strip().replace(' ', '')
                results.append({"address": ip, "name_suffix": isp})  # ISPä½œä¸ºåç§°
        print(f"âœ… è·å– {len(results)} ä¸ªåŠ¨æ€IP")
        return results
    except Exception as e:
        print(f"âŒ åŠ¨æ€IPè·å–å¤±è´¥: {e}")
        return []

def fetch_github_ips(url):
    """ä» GitHub Raw è§£æå¸¦ç«¯å£å’Œåç§°çš„IP"""
    print("ğŸ”„ è·å–GitHubä¼˜é€‰IP...")
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        regex = r'^([^:]+):(\d+)#(.*)$'
        results = []
        for line in r.text.strip().split("\n"):
            m = re.match(regex, line.strip())
            if m:
                ip, port, name = m.groups()
                results.append({"address": f"{ip}:{port}", "name_suffix": name.strip() or ip})
        print(f"âœ… è·å– {len(results)} ä¸ªGitHubIP")
        return results
    except Exception as e:
        print(f"âŒ GitHub IP è·å–å¤±è´¥: {e}")
        return []

def generate_subscription():
    """ç”Ÿæˆè®¢é˜…æ–‡ä»¶"""
    try:
        with open(TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            tpl = f.read().strip()
        uuid = re.search(r'vless://([^@]+)@', tpl).group(1)
        params = re.search(r'\?(.+)', tpl).group(1)
        params = re.sub(r'host=[^&]+', f'host={FIXED_SNI_HOST}', params) if "host=" in params else params + f"&host={FIXED_SNI_HOST}"
        params = re.sub(r'sni=[^&]+', f'sni={FIXED_SNI_HOST}', params) if "sni=" in params else params + f"&sni={FIXED_SNI_HOST}"
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶ {TEMPLATE_FILE}")
        return

    print("\n--- è·å–æ‰€æœ‰èŠ‚ç‚¹ ---")
    all_nodes = (
        fetch_from_file(DOMAINS_FILE) +
        fetch_simple_ips(REMOTE_IP_URL_1) +
        fetch_dynamic_ips(DYNAMIC_IP_URL) +
        fetch_github_ips(GITHUB_IP_URL) +
        fetch_simple_ips(EXTRA_IP_URL)  # æ–°å¢æ¥æº
    )
    if not all_nodes:
        print("âš ï¸ æ²¡æœ‰èŠ‚ç‚¹ï¼Œé€€å‡º")
        return

    print("\nğŸš€ ç”ŸæˆèŠ‚ç‚¹é“¾æ¥...")
    node_links = []
    for i, node in enumerate(all_nodes, start=1):
        address = node["address"]
        name_suffix = node.get("name_suffix", address)  # åŸåç§°ï¼Œä¸åŠ  CF- ä¸æŸ¥åœ°åŒº
        server_address = address if ":" in address and not re.search(r'[a-zA-Z]', address.split(':')[0]) else f"{address}:443"
        link = f"vless://{uuid}@{server_address}?{params}"
        node_name = f"{name_suffix}-{i:03d}"  # åç§° + åºå·
        node_links.append(f"{link}#{quote(node_name)}")

    print(f"ğŸ‰ ç”Ÿæˆ {len(node_links)} èŠ‚ç‚¹")
    encoded_content = base64.b64encode("\n".join(node_links).encode("utf-8")).decode("utf-8")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(encoded_content)
    print(f"âœ¨ å·²å†™å…¥ {OUTPUT_FILE}")

if __name__ == "__main__":
    generate_subscription()
